{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPunX2wAVOvpXg/M1fKnpss",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rakshitha149/NLP-AD/blob/main/4_3_25_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import atexit\n",
        "\n",
        "import shutil\n",
        "\n",
        "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
        "\n",
        "# Step 1: Load the pre-trained BlenderBot model and tokenizer\n",
        "\n",
        "model_name = \"facebook/blenderbot-1B-distill\"\n",
        "\n",
        "tokenizer = BlenderbotTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = BlenderbotForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Step 2: Define a function to interact with the chatbot\n",
        "\n",
        "def interact_with_chatbot(user_input, conversation_history):\n",
        "\n",
        "    # Step 2.1: Add user input to the conversation history\n",
        "\n",
        "    conversation_history.append(f\"User: {user_input}\")\n",
        "\n",
        "    # Step 2.2: Prepare the input text for the model\n",
        "\n",
        "    conversation_text = \" \".join(conversation_history[-5:])  # Use only the last 5 exchanges to keep context manageable\n",
        "\n",
        "    # Step 2.3: Generate a response using the chatbot pipeline\n",
        "\n",
        "    inputs = tokenizer(conversation_text, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "    response_ids = model.generate(**inputs, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    response_text = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Step 2.4: Add the generated response to the conversation history\n",
        "\n",
        "    conversation_history.append(f\"Chatbot: {response_text}\")\n",
        "\n",
        "# Step 3: Define a function to delete the model files from the cache directory\n",
        "\n",
        "def delete_model_files():\n",
        "\n",
        "    cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub/models--facebook--blenderbot-1B-distill\")\n",
        "\n",
        "    if os.path.exists(cache_dir):\n",
        "\n",
        "        user_input = input(\"Do you want to delete the model files from the cache directory? (y/n): \")\n",
        "\n",
        "        if user_input.lower() == 'y':\n",
        "\n",
        "            shutil.rmtree(cache_dir)\n",
        "\n",
        "            print(f\"Deleted model files from cache directory: {cache_dir}\")\n",
        "\n",
        "        else:\n",
        "\n",
        "            print(\"Model files not deleted from cache directory.\")\n",
        "\n",
        "    else:\n",
        "\n",
        "        print(f\"Model files not found in cache directory: {cache_dir}\")\n",
        "\n",
        "#step 4\n",
        "atexit.register(delete_model_files)\n",
        "\n",
        "# Step 5: Start the conversation loop\n",
        "\n",
        "print(\"Welcome to the Indian Tourism Chatbot!\")\n",
        "\n",
        "print(\"Type 'quit' to end the conversation.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3r6WBcVxTxnk",
        "outputId": "c6b450e2-55ad-4251-9530-8a0ae335c8d4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the Indian Tourism Chatbot!\n",
            "Type 'quit' to end the conversation.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import atexit\n",
        "\n",
        "import shutil\n",
        "\n",
        "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
        "\n",
        "# Step 1: Load the pre-trained BlenderBot model and tokenizer\n",
        "\n",
        "model_name = \"facebook/blenderbot-1B-distill\"\n",
        "\n",
        "tokenizer = BlenderbotTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = BlenderbotForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "Mu0m951HaMzl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Define a function to interact with the chatbot\n",
        "\n",
        "def interact_with_chatbot(user_input, conversation_history):\n",
        "\n",
        "    # Step 2.1: Add user input to the conversation history\n",
        "\n",
        "    conversation_history.append(f\"User: {user_input}\")\n",
        "\n",
        "    # Step 2.2: Prepare the input text for the model\n",
        "\n",
        "    conversation_text = \" \".join(conversation_history[-5:])  # Use only the last 5 exchanges to keep context manageable\n",
        "\n",
        "    # Step 2.3: Generate a response using the chatbot pipeline\n",
        "\n",
        "    inputs = tokenizer(conversation_text, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "    response_ids = model.generate(**inputs, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    response_text = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Step 2.4: Add the generated response to the conversation history\n",
        "\n",
        "    conversation_history.append(f\"Chatbot: {response_text}\")\n"
      ],
      "metadata": {
        "id": "zpuigZirTxk5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define a function to delete the model files from the cache directory\n",
        "\n",
        "def delete_model_files():\n",
        "\n",
        "    cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub/models--facebook--blenderbot-1B-distill\")\n",
        "\n",
        "    if os.path.exists(cache_dir):\n",
        "\n",
        "        user_input = input(\"Do you want to delete the model files from the cache directory? (y/n): \")\n",
        "\n",
        "        if user_input.lower() == 'y':\n",
        "\n",
        "            shutil.rmtree(cache_dir)\n",
        "\n",
        "            print(f\"Deleted model files from cache directory: {cache_dir}\")\n",
        "\n",
        "        else:\n",
        "\n",
        "            print(\"Model files not deleted from cache directory.\")\n",
        "\n",
        "    else:\n",
        "\n",
        "        print(f\"Model files not found in cache directory: {cache_dir}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kEgVh6HTXbn7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Register the delete_model_files function to be called on program exit\n",
        "\n",
        "atexit.register(delete_model_files)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "ISpqZbFKXbcn",
        "outputId": "7f0d8355-f84e-421f-f5b6-a86c1d0378c2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.delete_model_files()>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>delete_model_files</b><br/>def delete_model_files()</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/&lt;ipython-input-7-988e840d9459&gt;</a>&lt;no docstring&gt;</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Start the conversation loop\n",
        "\n",
        "print(\"Welcome to the Indian Tourism Chatbot!\")\n",
        "\n",
        "print(\"Type 'quit' to end the conversation.\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuHOPInZaFcp",
        "outputId": "905541c3-8c85-4cff-d881-e6c25268d3c5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the Indian Tourism Chatbot!\n",
            "Type 'quit' to end the conversation.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_history = []\n",
        "\n",
        "while True:\n",
        "\n",
        "    # Step 5.1: Get user input\n",
        "\n",
        "    user_input = input(\"User: \")\n",
        "\n",
        "    # Step 5.2: Check if the user wants to quit\n",
        "\n",
        "    if user_input.lower() == 'quit':\n",
        "\n",
        "        print(\"Thank you for using the Indian Tourism Chatbot. Goodbye!\")\n",
        "\n",
        "        break\n",
        "\n",
        "    # Step 5.3: Generate and print the chatbot's response\n",
        "\n",
        "    response = interact_with_chatbot(user_input, conversation_history)\n",
        "\n",
        "    print(f\"Chatbot: {response}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhdI4hxzaFZJ",
        "outputId": "0f6e6544-03b2-42e7-fd20-9d06182e8fe8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: quit\n",
            "Thank you for using the Indian Tourism Chatbot. Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sdJ3aphsaLVk"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}